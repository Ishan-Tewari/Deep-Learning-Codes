{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Practical 7 RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ypdJjIrXuvb"
      },
      "source": [
        "# Language Modelling using RNN \n",
        "\n",
        "# i) Probability Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rmas061XzUg"
      },
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YBuPNVq-bIT"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, Embedding\n",
        "\n",
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvIcm2_SY_zu"
      },
      "source": [
        "## Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfc1oKxNYNyr",
        "outputId": "4db3bdf3-3f3a-46cf-fabb-09230dc8dd53"
      },
      "source": [
        "data = \"\"\" Jack and Jill went up the hill .\\n To fetch a pail of water .\\n Jack fell down and broke his crown .\\n And Jill came tumbling after . \"\"\"\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Jack and Jill went up the hill .\n",
            " To fetch a pail of water .\n",
            " Jack fell down and broke his crown .\n",
            " And Jill came tumbling after . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY8zOeRzZTZ6",
        "outputId": "bb76b9dc-5656-4440-f54d-55e10e8adea1"
      },
      "source": [
        "data_splitted = data.split('\\n')\n",
        "data_splitted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Jack and Jill went up the hill .',\n",
              " ' To fetch a pail of water .',\n",
              " ' Jack fell down and broke his crown .',\n",
              " ' And Jill came tumbling after . ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwKaBXW1aHzp"
      },
      "source": [
        "Adding filters except '.' as we dont want '.' to be filtered out by the tokenizer but be considered as a token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIwMcrFFZjOk",
        "outputId": "a9f990d8-fc39-4ebe-a63c-dc60d1a63ec7"
      },
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~')\n",
        "\n",
        "# Initializing vocabulary\n",
        "tokenizer.fit_on_texts(data_splitted)\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "vocab_length = len(tokenizer.word_index) + 1\n",
        "vocab_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'.': 1, 'and': 2, 'jack': 3, 'jill': 4, 'went': 5, 'up': 6, 'the': 7, 'hill': 8, 'to': 9, 'fetch': 10, 'a': 11, 'pail': 12, 'of': 13, 'water': 14, 'fell': 15, 'down': 16, 'broke': 17, 'his': 18, 'crown': 19, 'came': 20, 'tumbling': 21, 'after': 22}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwvYLePHbLzb"
      },
      "source": [
        "**Converting text to numerical sequences based on word index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyNq_qouahhj",
        "outputId": "1409b022-5fbc-4860-ce50-d24c5a7db91c"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(data_splitted)\n",
        "sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 2, 4, 5, 6, 7, 8, 1],\n",
              " [9, 10, 11, 12, 13, 14, 1],\n",
              " [3, 15, 16, 2, 17, 18, 19, 1],\n",
              " [2, 4, 20, 21, 22, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJaE8lyYbjGd"
      },
      "source": [
        "**Removing last word index from input as it is not used in the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpwDwuFBbf4p",
        "outputId": "bf96d4f0-1888-4e61-fe91-c58959f267aa"
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(len(sequences)):\n",
        "  X.append(sequences[i][:-1])\n",
        "\n",
        "y = sequences\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 2, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14], [3, 15, 16, 2, 17, 18, 19], [2, 4, 20, 21, 22]]\n",
            "[[3, 2, 4, 5, 6, 7, 8, 1], [9, 10, 11, 12, 13, 14, 1], [3, 15, 16, 2, 17, 18, 19, 1], [2, 4, 20, 21, 22, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA1FxPpcfO_X"
      },
      "source": [
        "**Adding 0 as x<0> and y<0> for all inputs and outputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztd8bzxre9Na",
        "outputId": "0ed785b7-86e4-4dbc-e049-1c5611e6e70c"
      },
      "source": [
        "for x in X:\n",
        "  x.insert(0,0)\n",
        "\n",
        "for op in y:\n",
        "  op.insert(0,0)\n",
        "\n",
        "X\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 3, 2, 4, 5, 6, 7, 8, 1],\n",
              " [0, 9, 10, 11, 12, 13, 14, 1],\n",
              " [0, 3, 15, 16, 2, 17, 18, 19, 1],\n",
              " [0, 2, 4, 20, 21, 22, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As6APCDVfy8R"
      },
      "source": [
        "**Making every input of the same length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDr2TCWHfvof",
        "outputId": "72902dff-0e1d-4faf-9790-4e0d8e80c1c8"
      },
      "source": [
        "# finding the max length of input sequences\n",
        "max_len = 0; \n",
        "for x in X:\n",
        "  max_len = max(max_len,len(x))\n",
        "\n",
        "max_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-RHwikDf7y5"
      },
      "source": [
        "**Padding X with 0 to make length of all input same**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vKIw7C2fcmf",
        "outputId": "f5ecdb97-0728-4b7f-ea9c-dc8f357e868b"
      },
      "source": [
        "X = pad_sequences(X,max_len,padding='pre')\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  3,  2,  4,  5,  6,  7,  8],\n",
              "       [ 0,  0,  9, 10, 11, 12, 13, 14],\n",
              "       [ 0,  3, 15, 16,  2, 17, 18, 19],\n",
              "       [ 0,  0,  0,  2,  4, 20, 21, 22]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clENHZZKgDSc"
      },
      "source": [
        "**Adding 0 to y because of padding in X**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTGPH-E3f1ML",
        "outputId": "1924b647-bbb0-4a41-add7-24557e92c8f2"
      },
      "source": [
        "y = pad_sequences(y,max_len,padding='pre')\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  2,  4,  5,  6,  7,  8,  1],\n",
              "       [ 0,  9, 10, 11, 12, 13, 14,  1],\n",
              "       [ 3, 15, 16,  2, 17, 18, 19,  1],\n",
              "       [ 0,  0,  2,  4, 20, 21, 22,  1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waUK77KDiYQs"
      },
      "source": [
        "**Converting to One-Hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOBam8uwg8Y_",
        "outputId": "4b6d3792-7cbc-4784-83df-a445c61ca86b"
      },
      "source": [
        "y = to_categorical(y,num_classes = vocab_length)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Quo9wKEPihM9",
        "outputId": "6dd2538b-a800-4810-d140-51a0147d9c8b"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 8, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIEw3D0flWjj"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPpRnnW-jEnQ",
        "outputId": "c92f37bd-921f-4099-839a-dae8629f419f"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_length, output_dim=10))\n",
        "model.add(SimpleRNN(50,return_sequences=True))\n",
        "model.add(Dense(units=vocab_length, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 10)          230       \n",
            "_________________________________________________________________\n",
            "simple_rnn_4 (SimpleRNN)     (None, None, 50)          3050      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, None, 23)          1173      \n",
            "=================================================================\n",
            "Total params: 4,453\n",
            "Trainable params: 4,453\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u_TsuzXncPn"
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SrH3xPVoFzr",
        "outputId": "85cc62a4-cd11-49b7-cee8-dca12a72f8c7"
      },
      "source": [
        "model.fit(X,y,epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.1384 - accuracy: 0.0000e+00\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.1055 - accuracy: 0.0938\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.0775 - accuracy: 0.1562\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.0463 - accuracy: 0.2188\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.0096 - accuracy: 0.1875\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9669 - accuracy: 0.1875\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9190 - accuracy: 0.1875\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8678 - accuracy: 0.1875\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.8154 - accuracy: 0.1875\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.7631 - accuracy: 0.2188\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.7121 - accuracy: 0.2188\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6638 - accuracy: 0.2188\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6190 - accuracy: 0.2812\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5776 - accuracy: 0.2500\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.5392 - accuracy: 0.2500\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5030 - accuracy: 0.2500\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4685 - accuracy: 0.2500\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4354 - accuracy: 0.2500\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4033 - accuracy: 0.2500\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3721 - accuracy: 0.2500\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3417 - accuracy: 0.2812\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3121 - accuracy: 0.3125\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2831 - accuracy: 0.3125\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.2547 - accuracy: 0.3125\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.2269 - accuracy: 0.3125\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.1997 - accuracy: 0.3438\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1731 - accuracy: 0.3750\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1469 - accuracy: 0.4062\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1213 - accuracy: 0.4062\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.0962 - accuracy: 0.4062\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0715 - accuracy: 0.4062\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 2.0473 - accuracy: 0.4375\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0235 - accuracy: 0.4375\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0001 - accuracy: 0.4688\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.9771 - accuracy: 0.5000\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9544 - accuracy: 0.5312\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9320 - accuracy: 0.5312\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9100 - accuracy: 0.5312\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8882 - accuracy: 0.5625\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8666 - accuracy: 0.5625\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8453 - accuracy: 0.5625\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8242 - accuracy: 0.5625\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8032 - accuracy: 0.5625\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.7824 - accuracy: 0.5625\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7617 - accuracy: 0.5625\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7412 - accuracy: 0.5938\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7207 - accuracy: 0.6250\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7006 - accuracy: 0.6250\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6813 - accuracy: 0.6250\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6641 - accuracy: 0.6562\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6467 - accuracy: 0.6250\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6275 - accuracy: 0.6875\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6067 - accuracy: 0.6250\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5874 - accuracy: 0.6875\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5685 - accuracy: 0.6562\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5500 - accuracy: 0.6562\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5316 - accuracy: 0.6562\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5134 - accuracy: 0.6562\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4953 - accuracy: 0.6562\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4772 - accuracy: 0.6562\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4591 - accuracy: 0.6875\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4411 - accuracy: 0.6875\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4232 - accuracy: 0.6875\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4054 - accuracy: 0.7500\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3876 - accuracy: 0.7188\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3700 - accuracy: 0.8125\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.3524 - accuracy: 0.8438\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3350 - accuracy: 0.8438\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3177 - accuracy: 0.8438\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.3007 - accuracy: 0.8438\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2839 - accuracy: 0.8438\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.2673 - accuracy: 0.8438\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2507 - accuracy: 0.8438\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2342 - accuracy: 0.8438\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2175 - accuracy: 0.8438\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2011 - accuracy: 0.8438\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1846 - accuracy: 0.8438\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1685 - accuracy: 0.8438\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1524 - accuracy: 0.8438\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1365 - accuracy: 0.8438\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1207 - accuracy: 0.8438\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1050 - accuracy: 0.8438\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0894 - accuracy: 0.8125\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0739 - accuracy: 0.8125\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0586 - accuracy: 0.8125\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0433 - accuracy: 0.8125\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0282 - accuracy: 0.8125\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0132 - accuracy: 0.8125\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.9984 - accuracy: 0.8125\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9838 - accuracy: 0.8438\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.9692 - accuracy: 0.8438\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9549 - accuracy: 0.8438\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.9406 - accuracy: 0.8438\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9265 - accuracy: 0.8438\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9125 - accuracy: 0.8438\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8987 - accuracy: 0.8438\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8850 - accuracy: 0.8438\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8716 - accuracy: 0.8750\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8583 - accuracy: 0.8438\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8452 - accuracy: 0.8750\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8323 - accuracy: 0.8750\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8197 - accuracy: 0.8750\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8072 - accuracy: 0.8750\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7950 - accuracy: 0.8750\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7829 - accuracy: 0.8750\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7711 - accuracy: 0.8750\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7594 - accuracy: 0.8750\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7479 - accuracy: 0.8750\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7366 - accuracy: 0.8750\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7255 - accuracy: 0.8750\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7146 - accuracy: 0.8750\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7038 - accuracy: 0.8750\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6933 - accuracy: 0.8750\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6829 - accuracy: 0.8750\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6727 - accuracy: 0.8750\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6626 - accuracy: 0.8750\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6528 - accuracy: 0.8750\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6431 - accuracy: 0.8750\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6336 - accuracy: 0.8750\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6243 - accuracy: 0.8750\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6152 - accuracy: 0.8750\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6062 - accuracy: 0.8750\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5974 - accuracy: 0.8750\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5887 - accuracy: 0.8750\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5802 - accuracy: 0.8750\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5719 - accuracy: 0.8750\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5637 - accuracy: 0.8750\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5556 - accuracy: 0.8750\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5477 - accuracy: 0.8750\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5400 - accuracy: 0.8750\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5323 - accuracy: 0.8750\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5249 - accuracy: 0.8750\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5175 - accuracy: 0.8750\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5103 - accuracy: 0.8750\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5033 - accuracy: 0.8750\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4963 - accuracy: 0.8750\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4895 - accuracy: 0.8750\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4828 - accuracy: 0.8750\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4763 - accuracy: 0.8750\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4699 - accuracy: 0.8750\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4636 - accuracy: 0.8750\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4574 - accuracy: 0.8750\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4513 - accuracy: 0.8750\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4453 - accuracy: 0.8750\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4395 - accuracy: 0.8750\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4337 - accuracy: 0.8750\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4281 - accuracy: 0.8750\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4225 - accuracy: 0.8750\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4171 - accuracy: 0.8750\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4118 - accuracy: 0.8750\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4065 - accuracy: 0.8750\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4014 - accuracy: 0.8750\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3964 - accuracy: 0.8750\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3914 - accuracy: 0.8750\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3865 - accuracy: 0.8750\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3818 - accuracy: 0.8750\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3771 - accuracy: 0.8750\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3725 - accuracy: 0.8750\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3680 - accuracy: 0.8750\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.3636 - accuracy: 0.8750\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3593 - accuracy: 0.8750\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3551 - accuracy: 0.8750\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3509 - accuracy: 0.8750\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3469 - accuracy: 0.8750\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3429 - accuracy: 0.8750\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3390 - accuracy: 0.8750\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3352 - accuracy: 0.8750\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3315 - accuracy: 0.8750\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3278 - accuracy: 0.8750\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3242 - accuracy: 0.8750\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3207 - accuracy: 0.8750\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3172 - accuracy: 0.8750\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3139 - accuracy: 0.8750\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3106 - accuracy: 0.8750\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3074 - accuracy: 0.8750\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3042 - accuracy: 0.8750\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3012 - accuracy: 0.8750\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2982 - accuracy: 0.8750\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2953 - accuracy: 0.8750\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2924 - accuracy: 0.8750\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2896 - accuracy: 0.8750\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2868 - accuracy: 0.8750\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2841 - accuracy: 0.8750\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2815 - accuracy: 0.8750\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2788 - accuracy: 0.8750\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2763 - accuracy: 0.8750\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2738 - accuracy: 0.8750\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2714 - accuracy: 0.8750\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2690 - accuracy: 0.8750\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2667 - accuracy: 0.8750\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2644 - accuracy: 0.8750\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2622 - accuracy: 0.8750\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2601 - accuracy: 0.8750\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.2580 - accuracy: 0.8750\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2559 - accuracy: 0.8750\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2538 - accuracy: 0.8750\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2518 - accuracy: 0.8750\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2499 - accuracy: 0.8750\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2480 - accuracy: 0.8750\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2461 - accuracy: 0.8750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd825bed9d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QsKHRIbqKLt"
      },
      "source": [
        "## Function for predicting probability of a Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxTnPMcLoNtn"
      },
      "source": [
        "def prob_of_sentence(model,tokenizer,sentence):\n",
        "\n",
        "  # converting sentence into numerical form\n",
        "  encoded_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "  print(encoded_sentence)\n",
        "\n",
        "  # adding 0 as X<0>\n",
        "  encoded_sentence.insert(0,0)\n",
        "  print(encoded_sentence)\n",
        "\n",
        "  encoded_sentence = np.array(encoded_sentence).reshape((1,-1))\n",
        "  print(encoded_sentence)\n",
        "\n",
        "  prob = model.predict_proba(encoded_sentence)\n",
        "  print(prob.shape)\n",
        "\n",
        "  probability = 1\n",
        "  for i in range(0,prob.shape[1] - 1):\n",
        "    probability *= prob[0,i,encoded_sentence[0,i+1]]\n",
        "  print(probability)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu5UtX5wxyuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357e2a59-a69e-4a29-f04b-374ac9466fc7"
      },
      "source": [
        "prob_of_sentence(model,tokenizer,\"Jack and Jill .\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 2, 4, 1]\n",
            "[0, 3, 2, 4, 1]\n",
            "[[0 3 2 4 1]]\n",
            "(1, 5, 23)\n",
            "1.8183255523435641e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvgHxHCWOmAR"
      },
      "source": [
        "## ii) Sentence Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etf93S9IgLcC"
      },
      "source": [
        "## Random sentence generation without seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGDmZsdvhZ8A"
      },
      "source": [
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSzwd_aqx2XL"
      },
      "source": [
        "def sample_all_wo_seed(model,tokenizer,n_words,vocab_length):\n",
        "  encoded_sentence = []\n",
        "  inp_text = ''\n",
        "\n",
        "  for i in range(n_words):\n",
        "    print('-'*50)\n",
        "    print('Input text : ', inp_text)\n",
        "\n",
        "    # converting sentence into numerical form\n",
        "    encoded_sentence = tokenizer.texts_to_sequences([inp_text])[0]\n",
        "\n",
        "    # adding 0 as X<0>\n",
        "    encoded_sentence.insert(0,0)\n",
        "    \n",
        "    encoded_sentence = np.array(encoded_sentence).reshape((1,-1))\n",
        "    print(\"For i : {} Encoded is : {}\".format(i, encoded_sentence))\n",
        "\n",
        "    if i == 0:\n",
        "      prob = model.predict_proba(encoded_sentence, verbose= 0)\n",
        "      y_hat = 0\n",
        "      while y_hat == 0:\n",
        "        y_hat = np.random.choice(range(vocab_length),p=prob.ravel())\n",
        "        y_hat = np.array(y_hat).reshape((1,-1))\n",
        "      print(\"For i : {} yhat in if is : {}\".format(i, y_hat))\n",
        "\n",
        "    else:\n",
        "      prob = model.predict_proba(encoded_sentence, verbose= 0)\n",
        "      print(prob.shape)\n",
        "      y_hat = np.append(y_hat,0)\n",
        "      y_hat = np.array(y_hat).reshape((1,-1))\n",
        "\n",
        "      while y_hat[0][i] == 0:\n",
        "        y_hat[0][i] = np.random.choice(range(vocab_length),p=prob[0][i].ravel())\n",
        "      print(\"For i : {} yhat in else is : {}\".format(i, y_hat))\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == y_hat[0][i]:\n",
        "        output_word = word\n",
        "        break\n",
        "    inp_text += output_word + ' '\n",
        "\n",
        "    print('-'*50)\n",
        "\n",
        "  return inp_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awgqP3OBZmDK",
        "outputId": "6236a7d9-28a4-4b38-bf5c-3c63a95c0fbd"
      },
      "source": [
        "print('\\n\\n' + color.BOLD + sample_all_wo_seed(model,tokenizer,3,vocab_length) + color.END)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Input text :  \n",
            "For i : 0 Encoded is : [[0]]\n",
            "For i : 0 yhat in if is : [[3]]\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Input text :  jack \n",
            "For i : 1 Encoded is : [[0 3]]\n",
            "(1, 2, 23)\n",
            "For i : 1 yhat in else is : [[3 2]]\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Input text :  jack and \n",
            "For i : 2 Encoded is : [[0 3 2]]\n",
            "(1, 3, 23)\n",
            "For i : 2 yhat in else is : [[ 3  2 16]]\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1mjack and down \u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCF_CN5mgP88"
      },
      "source": [
        "## Sentence generation with highest probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Q6SkHiZsfv"
      },
      "source": [
        "def sample_all_wo_seed_with_hp(model,tokenizer,n_words,vocab_length):\n",
        "  encoded_sentence = []\n",
        "  inp_text = ''\n",
        "\n",
        "  for i in range(n_words):\n",
        "    print('-'*50)\n",
        "    print('Input text : ', inp_text)\n",
        "\n",
        "    # converting sentence into numerical form\n",
        "    encoded_sentence = tokenizer.texts_to_sequences([inp_text])[0]\n",
        "\n",
        "    # adding 0 as X<0>\n",
        "    encoded_sentence.insert(0,0)\n",
        "    \n",
        "    encoded_sentence = np.array(encoded_sentence).reshape((1,-1))\n",
        "    print(\"For i : {} Encoded is : {}\".format(i, encoded_sentence))\n",
        "\n",
        "    if i == 0:\n",
        "      prob = model.predict_proba(encoded_sentence, verbose= 0)\n",
        "      y_hat = 0\n",
        "      while y_hat == 0:\n",
        "        y_hat = np.random.choice(range(vocab_length),p=prob.ravel())\n",
        "        y_hat = np.array(y_hat).reshape((1,-1))\n",
        "      print(\"For i : {} yhat in if is : {}\".format(i, y_hat))\n",
        "\n",
        "    else:\n",
        "      prob = model.predict_proba(encoded_sentence, verbose= 0)\n",
        "      print(prob.shape)\n",
        "      y_hat = np.append(y_hat,0)\n",
        "      y_hat = np.array(y_hat).reshape((1,-1))\n",
        "\n",
        "      # while y_hat[0][i] == 0:\n",
        "      \n",
        "      y_hat[0][i] = np.argmax(prob[0][i].ravel()[1:] , axis=0)\n",
        "      print(\"For i : {} yhat in else is : {}\".format(i, y_hat))\n",
        "\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == y_hat[0][i]:\n",
        "        output_word = word\n",
        "        break\n",
        "    inp_text += output_word + ' '\n",
        "\n",
        "    print('-'*50)\n",
        "\n",
        "  return inp_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0qxSHIggosN",
        "outputId": "d3b96f51-1e01-4714-c81c-8c10e2649fb8"
      },
      "source": [
        "print('\\n\\n' + color.BOLD + sample_all_wo_seed_with_hp(model,tokenizer,5,vocab_length) + color.END)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Input text :  \n",
            "For i : 0 Encoded is : [[0]]\n",
            "For i : 0 yhat in if is : [[3]]\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Input text :  jack \n",
            "For i : 1 Encoded is : [[0 3]]\n",
            "(1, 2, 23)\n",
            "For i : 1 yhat in else is : [[ 3 14]]\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Input text :  jack water \n",
            "For i : 2 Encoded is : [[ 0  3 14]]\n",
            "(1, 3, 23)\n",
            "For i : 2 yhat in else is : [[ 3 14  3]]\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Input text :  jack water jack \n",
            "For i : 3 Encoded is : [[ 0  3 14  3]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 4, 23)\n",
            "For i : 3 yhat in else is : [[ 3 14  3  4]]\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Input text :  jack water jack jill \n",
            "For i : 4 Encoded is : [[ 0  3 14  3  4]]\n",
            "(1, 5, 23)\n",
            "For i : 4 yhat in else is : [[ 3 14  3  4  5]]\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1mjack water jack jill went \u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS3JxSKNgpOu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}